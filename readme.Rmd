---
title: "Learning Docker"
output: github_document
---

```{r setup, include=FALSE}
Sys.setenv(PATH=paste0(Sys.getenv("PATH"), ":", getwd()))
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(error = TRUE)
```

## Introduction

![Build README](https://github.com/davetang/learning_docker/actions/workflows/create_readme.yml/badge.svg)

Docker is an open source project that allows one to pack, ship, and run any application as a lightweight container. An analogy of Docker containers are shipping containers, which provide a standard and consistent way of shipping just about anything. The container includes everything that is needed for an application to run including the code, system tools, and the necessary dependencies. If you wanted to test an application, all you need to do is to download the Docker image and run it in a new container. No more compiling and installing missing dependencies!

The [overview](https://docs.docker.com/get-started/overview/) at https://docs.docker.com/ provides more information. For more a more hands-on approach, check out know [Enough Docker to be Dangerous](https://docs.docker.com/) and [this short workshop](https://davetang.github.io/reproducible_bioinformatics/docker.html) that I prepared for BioC Asia 2019.

This README was generated by GitHub Actions using the R Markdown file `readme.Rmd`, which was executed via the `create_readme.sh` script.

## Installing the Docker Engine

To get started, you will need to install the Docker Engine; check out [this guide](https://docs.docker.com/engine/install/).

## Checking your installation

To see if everything is working, try to obtain the Docker version.

```{bash engine.opts='-l'}
docker --version
```

And run the `hello-world` image. (The `--rm` parameter is used to automatically remove the container when it exits.)

```{bash engine.opts='-l'}
docker run --rm hello-world
```

## Basics

The two guides linked in the introduction section provide some information on the basic commands but I'll include some here as well. One of the main reasons I use Docker is for building tools. For this purpose, I use Docker like a virtual machine, where I can install whatever I want. This is important because I can do my testing in an isolated environment and not worry about affecting the main server. I like to use Ubuntu because it's a popular Linux distribution and therefore whenever I run into a problem, chances are higher that someone else has had the same problem, asked a question on a forum, and received a solution.

Before we can run Ubuntu using Docker, we need an image. We can obtain an Ubuntu image from the [official Ubuntu image repository](https://hub.docker.com/_/ubuntu/) from Docker Hub by running `docker pull`.

```{bash engine.opts='-l'}
docker pull ubuntu:18.04
```

To run Ubuntu using Docker, we use `docker run`.

```{bash engine.opts='-l'}
docker run --rm ubuntu:18.04 cat /etc/os-release
```

You can work interactively with the Ubuntu image by specifying the `-it` option.

```bash
docker run --rm -it ubuntu:18:04 /bin/bash
```

You may have noticed that I keep using the `--rm` option, which removes the container once you quit. If you don't use this option, the container is saved up until the point that you exit; all changes you made, files you created, etc. are saved. Why am I deleting all my changes? Because there is a better (and more reproducible) way to make changes to the system and that is by using a Dockerfile.

## Start containers automatically

When hosting a service using Docker (such as running [RStudio Server](https://davetang.org/muse/2021/04/24/running-rstudio-server-with-docker/https://davetang.org/muse/2021/04/24/running-rstudio-server-with-docker/)), it would be nice if the container automatically starts up again when the server (and Docker) restarts. If you use `--restart flag` with `docker run`, Docker will [restart your container](https://docs.docker.com/config/containers/start-containers-automatically/) when your container has exited or when Docker restarts. The value of the `--restart` flag can be the following:

* `no` - do not automatically restart (default)
* `on-failure[:max-retries]` - restarts if it exits due to an error (non-zero exit code) and the number of attempts is limited using the `max-retries` option
* `always` - always restarts the container; if it is manually stopped, it is restarted only when the Docker daemon restarts (or when the container is manually restarted)
* `unless-stopped` - similar to `always` but when the container is stopped, it is not restarted even after the Docker daemon restarts.

```bash
docker run --rm \
           -p 8888:8787 \
           -d \
           --restart always \
           -e PASSWORD=password \
           rocker/rstudio:4.0.5
```

## Dockerfile

A Dockerfile is a text file that contains instructions for building Docker images. A Dockerfile adheres to a specific format and set of instructions, which you can find at [Dockerfile reference](https://docs.docker.com/engine/reference/builder/). There is also a [Best practices guide](https://docs.docker.com/develop/develop-images/dockerfile_best-practices/) for writing Dockerfiles.

I have an example Dockerfile that uses the Ubuntu 18.04 image to build [BWA](https://github.com/lh3/bwa), a popular short read alignment tool used in bioinformatics.

```{bash engine.opts='-l'}
cat Dockerfile
```

## CMD

The [CMD](https://docs.docker.com/engine/reference/builder/#cmd) instruction in a Dockerfile does not execute anything at build time but specifies the intended command for the image; there can only be one CMD instruction in a Dockerfile and if you list more than one CMD then only the last CMD will take effect. The main purpose of a CMD is to provide defaults for an executing container.

## ENTRYPOINT

An [ENTRYPOINT](https://docs.docker.com/engine/reference/builder/#entrypoint) allows you to configure a container that will run as an executable. ENTRYPOINT has two forms:

* ENTRYPOINT ["executable", "param1", "param2"] (exec form, preferred)
* ENTRYPOINT command param1 param2 (shell form)

```bash
FROM ubuntu
ENTRYPOINT ["top", "-b"]
CMD ["-c"]
```

Use `--entrypoint` to override ENTRYPOINT instruction.

```bash
docker run --entrypoint
```

## Building an image

Use the `build` subcommand to build Docker images and use the `-f` parameter if your Dockerfile is named as something else otherwise Docker will look for a file named `Dockerfile`. The period at the end, tells Docker to look in the current directory.

```{bash engine.opts='-l'}
cat build.sh
```

You can push the built image to [Docker Hub](https://hub.docker.com/) if you have an account. I have used my Docker Hub account name to name my Docker image.

```bash
# use -f to specify the Dockerfile to use
# the period indicates that the Dockerfile is in the current directory
docker build -f Dockerfile.base -t davetang/base .

# log into Docker Hub
docker login

# push to Docker Hub
docker push davetang/base
```

## Renaming an image

Use `docker image tag`.

```bash
docker image tag old_image_name:latest new_image_name:latest
```

## Running an image

[Docker run documentation](https://docs.docker.com/engine/reference/run/).

```{bash engine.opts='-l'}
docker run --rm davetang/bwa:0.7.17
```

## Resource usage

To [restrict](https://docs.docker.com/config/containers/resource_constraints/) CPU usage use `--cpus=n` and use `--memory=` to restrict the maximum amount of memory the container can use.

We can confirm the limited CPU usage by running an endless while loop and using `docker stats` to confirm the CPU usage. *Remember to use `docker stop` to stop the container after confirming the usage!*

Restrict to 1 CPU.

```bash
# run in detached mode
docker run --rm -d --cpus=1 davetang/bwa:0.7.17 perl -le 'while(1){ }'

# check stats and use control+c to exit
docker stats
CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT   MEM %     NET I/O     BLOCK I/O   PIDS
8cc20bcfa4f4   vigorous_khorana   100.59%   572KiB / 1.941GiB   0.03%     736B / 0B   0B / 0B     1

docker stop 8cc20bcfa4f4
```

Restrict to 1/2 CPU.

```bash
# run in detached mode
docker run --rm -d --cpus=0.5 davetang/bwa:0.7.17 perl -le 'while(1){ }'

# check stats and use control+c to exit
docker stats

CONTAINER ID   NAME             CPU %     MEM USAGE / LIMIT   MEM %     NET I/O     BLOCK I/O   PIDS
af6e812a94da   unruffled_liskov   50.49%    584KiB / 1.941GiB   0.03%     736B / 0B   0B / 0B     1

docker stop af6e812a94da
```

## Copying files between host and container

Use `docker cp` but I recommend mounting a volume to a Docker container (see next section).

```bash
docker cp --help

Usage:  docker cp [OPTIONS] CONTAINER:SRC_PATH DEST_PATH|-
        docker cp [OPTIONS] SRC_PATH|- CONTAINER:DEST_PATH

Copy files/folders between a container and the local filesystem

Options:
  -L, --follow-link   Always follow symbol link in SRC_PATH
      --help          Print usage

# find container name
docker ps -a

# create file to transfer
echo hi > hi.txt

docker cp hi.txt fee424ef6bf0:/root/

# start container
docker start -ai fee424ef6bf0

# inside container
cat /root/hi.txt 
hi

# create file inside container
echo bye > /root/bye.txt
exit

# transfer file from container to host
docker cp fee424ef6bf0:/root/bye.txt .

cat bye.txt 
bye
```

## Sharing between host and container

Use the `-v` flag to mount directories to a container so that you can share files between the host and container.

In the example below, I am mounting `data` from the current directory (using the Unix command `pwd`) to `/work` in the container. I am working from the root directory of this GitHub repository, which contains the `data` directory.

```{bash engine.opts='-l'}
ls data
```

Any output written to `/work` inside the container, will be accessible inside `data` on the host. The command below will create BWA index files for `data/chrI.fa.gz`.

```{bash engine.opts='-l'}
docker run --rm -v $(pwd)/data:/work davetang/bwa:0.7.17 bwa index chrI.fa.gz
```

We can see the newly created index files.

```{bash engine.opts='-l'}
ls -lrt data
```

### File permissions

On newer version of Docker, you no longer have to worry about this. However, if you find that the file created inside your container on a mounted volume are owned by `root`, read on.

The files created inside the Docker container will be owned by root; inside the Docker container, you are `root` and the files you produce will have `root` permissions. 

```bash
ls -lrt
total 2816
-rw-r--r-- 1 1211 1211 1000015 Apr 27 02:00 ref.fa
-rw-r--r-- 1 1211 1211   21478 Apr 27 02:00 l100_n100_d400_31_2.fq
-rw-r--r-- 1 1211 1211   21478 Apr 27 02:00 l100_n100_d400_31_1.fq
-rw-r--r-- 1 1211 1211     119 Apr 27 02:01 run.sh
-rw-r--r-- 1 root root 1000072 Apr 27 02:03 ref.fa.bwt
-rw-r--r-- 1 root root  250002 Apr 27 02:03 ref.fa.pac
-rw-r--r-- 1 root root      40 Apr 27 02:03 ref.fa.ann
-rw-r--r-- 1 root root      12 Apr 27 02:03 ref.fa.amb
-rw-r--r-- 1 root root  500056 Apr 27 02:03 ref.fa.sa
-rw-r--r-- 1 root root   56824 Apr 27 02:04 aln.sam
```

This is problematic because when you're back in the host environment, you can't modify these files. To circumvent this, create a user that matches the host user by passing three environmental variables from the host to the container.

```bash
docker run -it \
-v ~/my_data:/data \
-e MYUID=`id -u` \
-e MYGID=`id -g` \
-e ME=`whoami` \
bwa /bin/bash
```

Use the steps below to create an identical user inside the container.

```bash
adduser --quiet --home /home/san/$ME --no-create-home --gecos "" --shell /bin/bash --disabled-password $ME

# optional: give yourself admin privileges
echo "%$ME ALL=(ALL) NOPASSWD:ALL" >> /etc/sudoers

# update the IDs to those passed into Docker via environment variable
sed -i -e "s/1000:1000/$MYUID:$MYGID/g" /etc/passwd
sed -i -e "s/$ME:x:1000/$ME:x:$MYGID/" /etc/group

# su - as the user
exec su - $ME

# run BWA again, after you have deleted the old files as root
bwa index ref.fa
bwa mem ref.fa l100_n100_d400_31_1.fq l100_n100_d400_31_2.fq > aln.sam

# check output
ls -lrt
total 2816
-rw-r--r-- 1 dtang dtang 1000015 Apr 27 02:00 ref.fa
-rw-r--r-- 1 dtang dtang   21478 Apr 27 02:00 l100_n100_d400_31_2.fq
-rw-r--r-- 1 dtang dtang   21478 Apr 27 02:00 l100_n100_d400_31_1.fq
-rw-r--r-- 1 dtang dtang     119 Apr 27 02:01 run.sh
-rw-rw-r-- 1 dtang dtang 1000072 Apr 27 02:12 ref.fa.bwt
-rw-rw-r-- 1 dtang dtang  250002 Apr 27 02:12 ref.fa.pac
-rw-rw-r-- 1 dtang dtang      40 Apr 27 02:12 ref.fa.ann
-rw-rw-r-- 1 dtang dtang      12 Apr 27 02:12 ref.fa.amb
-rw-rw-r-- 1 dtang dtang  500056 Apr 27 02:12 ref.fa.sa
-rw-rw-r-- 1 dtang dtang   56824 Apr 27 02:12 aln.sam

# exit container
exit
```

The files will be saved in `~/my_data` on the host.

```bash
ls -lrt ~/my_data
total 2816
-rw-r--r-- 1 dtang dtang 1000015 Apr 27 10:00 ref.fa
-rw-r--r-- 1 dtang dtang   21478 Apr 27 10:00 l100_n100_d400_31_2.fq
-rw-r--r-- 1 dtang dtang   21478 Apr 27 10:00 l100_n100_d400_31_1.fq
-rw-r--r-- 1 dtang dtang     119 Apr 27 10:01 run.sh
-rw-rw-r-- 1 dtang dtang 1000072 Apr 27 10:12 ref.fa.bwt
-rw-rw-r-- 1 dtang dtang  250002 Apr 27 10:12 ref.fa.pac
-rw-rw-r-- 1 dtang dtang      40 Apr 27 10:12 ref.fa.ann
-rw-rw-r-- 1 dtang dtang      12 Apr 27 10:12 ref.fa.amb
-rw-rw-r-- 1 dtang dtang  500056 Apr 27 10:12 ref.fa.sa
-rw-rw-r-- 1 dtang dtang   56824 Apr 27 10:12 aln.sam
```

### File Permissions 2

An easier way to set file permissions is to use the `-u` parameter.

```bash
# assuming blah.fa exists in /local/data/
docker run -v /local/data:/data -u `stat -c "%u:%g" /local/data` bwa bwa index /data/blah.fa
```

### Read only

To mount a volume but with read-only permissions, append `:ro` at the end.

```{bash engine.opts='-l'}
docker run --rm -v $(pwd):/work:ro davetang/bwa:0.7.17 touch test.txt
```

## Removing the image

Use `docker rmi` to remove an image. You will need to remove any stopped containers first before you can remove an image. Use `docker ps -a` to find stopped containers and `docker rm` to remove these containers.

Let's pull the `busybox` image.

```{bash engine.opts='-l'}
docker pull busybox
```

Check out `busybox`.

```{bash engine.opts='-l'}
docker images busybox
```

Remove `busybox`.

```{bash engine.opts='-l'}
docker rmi busybox
```

## Committing changes

Generally, it is better to use a Dockerfile to manage your images in a documented and maintainable way but if you still want to [commit changes](https://docs.docker.com/engine/reference/commandline/commit/) to your container (like you would for Git), read on.

When you log out of a container, the changes made are still stored; type `docker ps -a` to see all containers and the latest changes. Use `docker commit` to commit your changes.

```bash
docker ps -a

# git style commit
# -a, --author=       Author (e.g., "John Hannibal Smith <hannibal@a-team.com>")
# -m, --message=      Commit message
docker commit -m 'Made change to blah' -a 'Dave Tang' <CONTAINER ID> <image>

# use docker history <image> to check history
docker history <image>
```

## Access running container

To access a container that is already running, perhaps in the background (using detached mode: `docker run` with `-d`) use `docker ps` to find the name of the container and then use `docker exec`.

In the example below, my container name is `rstudio_dtang`.

```bash
docker exec -it rstudio_dtang /bin/bash
```

## Cleaning up exited containers

I typically use the `--rm` flag with `docker run` so that containers are automatically removed after I exit them. However, if you don't use `--rm`, by default a container's file system persists even after the container exits. For example:

```{bash engine.opts='-l'}
docker run hello-world
```

Show all containers.

```{bash engine.opts='-l'}
docker ps -a
```

We can use a sub-shell to get all (`-a`) container IDs (`-q`) that have exited (`-f status=exited`) and then remove them (`docker rm -v`).

```{bash engine.opts='-l'}
docker rm -v $(docker ps -a -q -f status=exited)
```

Check to see if the container still exists.

```{bash engine.opts='-l'}
docker ps -a
```

We can set this up as a Bash script so that we can easily remove exited containers. In the Bash script `-z` returns true if `$exited` is empty, i.e. no exited containers, so we will only run the command when `$exited` is not true.

```{bash engine.opts='-l'}
cat clean_up_docker.sh
```

As I have mentioned, you can use the [--rm](https://docs.docker.com/engine/reference/run/#clean-up---rm) parameter to automatically clean up the container and remove the file system when the container exits.

```{bash engine.opts='-l'}
docker run --rm hello-world
```

No containers.

```{bash engine.opts='-l'}
docker ps -a
```

## Installing Perl modules

Use `cpanminus`.

```bash
apt-get install -y cpanminus

# install some Perl modules
cpanm Archive::Extract Archive::Zip DBD::mysql
```

## Creating a data container

This [guide on working with Docker data volumes](https://www.digitalocean.com/community/tutorials/how-to-work-with-docker-data-volumes-on-ubuntu-14-04) provides a really nice introduction. Use `docker create` to create a data container; the `-v` indicates the directory for the data container; the `--name data_container` indicates the name of the data container; and `ubuntu` is the image to be used for the container.

```bash
docker create -v /tmp --name data_container ubuntu
```

If we run a new Ubuntu container with the `--volumes-from` flag, output written to the `/tmp` directory will be saved to the `/tmp` directory of the `data_container` container.

```bash
docker run -it --volumes-from data_container ubuntu /bin/bash
```

## R

Use images from [The Rocker Project](https://www.rocker-project.org/), for example `rocker/r-ver:4.1.0`.

```{bash engine.opts='-l'}
docker run --rm rocker/r-ver:4.1.0
```

## Saving and transferring a Docker image

You should just share the Dockerfile used to create your image but if you need another way to save and share an iamge, see [this post](http://stackoverflow.com/questions/23935141/how-to-copy-docker-images-from-one-host-to-another-without-via-repository) on Stack Overflow.

```bash
docker save -o <save image to path> <image name>
docker load -i <path to image tar file>
```

Here's an example.

```bash
# save on Unix server
docker save -o davebox.tar davebox

# copy file to MacBook Pro
scp davetang@192.168.0.31:/home/davetang/davebox.tar .

docker load -i davebox.tar 
93c22f563196: Loading layer [==================================================>] 134.6 MB/134.6 MB
...

docker images
REPOSITORY          TAG                 IMAGE ID            CREATED             SIZE
davebox             latest              d38f27446445        10 days ago         3.46 GB

docker run davebox samtools

Program: samtools (Tools for alignments in the SAM format)
Version: 1.3 (using htslib 1.3)

Usage:   samtools <command> [options]
...
```

## Pushing to Docker Hub

Create an account on [Docker Hub](https://hub.docker.com/); my account is `davetang`. Use `docker login` to login and use `docker push` to push to Docker Hub (run `docker tag` first if you didn't name your image in the format of `yourhubusername/newrepo`).

```bash
docker login

# create repo on Docker Hub then tag your image
docker tag bb38976d03cf yourhubusername/newrepo

# push
docker push yourhubusername/newrepo
```

## Tips

Tip from https://support.pawsey.org.au/documentation/display/US/Containers: each RUN, COPY, and ADD command in a Dockerfile generates another layer in the container thus increasing its size; use multi-line commands and clean up package manager caches to minimise image size:

```bash
RUN apt-get update \
      && apt-get install -y \
         autoconf \
         automake \
         gcc \
         g++ \
         python \
         python-dev \
      && apt-get clean all \
      && rm -rf /var/lib/apt/lists/*
```

I have found it handy to mount my current directory to the same path inside a Docker container and to [set it as the working directory](https://docs.docker.com/engine/reference/commandline/run/#set-working-directory--w); the directory will be automatically created inside the container if it does not already exist. When the container starts up, I will conveniently be in my current directory. In the command below I have also added the `-u` option, which sets the user to `<name|uid>[:<group|gid>]`.

```bash
docker run --rm -it -u $(stat -c "%u:%g" ${HOME}) -v $(pwd):$(pwd) -w $(pwd) davetang/build:1.1 /bin/bash
```

## Useful links

* [A quick introduction to Docker](http://blog.scottlowe.org/2014/03/11/a-quick-introduction-to-docker/)
* [The BioDocker project](https://github.com/BioDocker/biodocker); check out their [Wiki](https://github.com/BioDocker/biodocker/wiki), which has a lot of useful information
* [The impact of Docker containers on the performance of genomic pipelines](http://www.ncbi.nlm.nih.gov/pubmed/26421241)
* [Learn enough Docker to be useful](https://towardsdatascience.com/learn-enough-docker-to-be-useful-b0b44222eef5)
* [10 things to avoid in Docker containers](http://developers.redhat.com/blog/2016/02/24/10-things-to-avoid-in-docker-containers/)
* The [Play with Docker classroom](https://training.play-with-docker.com/) brings you labs and tutorials that help you get hands-on experience using Docker
* [Shifter](https://github.com/NERSC/shifter) enables container images for HPC
* http://biocworkshops2019.bioconductor.org.s3-website-us-east-1.amazonaws.com/page/BioconductorOnContainers__Bioconductor_Containers_Workshop/
